{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import DatasetPath\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import timm\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from skimage import io\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Dataset folder path\n",
    "DATA_DIR = DatasetPath.effectivePath\n",
    "\n",
    "# HYPERPARAMETRES\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "LR = 0.001\n",
    "\n",
    "EPOCHS = 50\n",
    "\n",
    "# Device to run calculations on \n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000\n",
      "4000\n",
      "(20000, 3)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(DATA_DIR + \"input.csv\")\n",
    "\n",
    "train_df, valid_df = train_test_split(df, test_size = 0.20, random_state = 42)\n",
    "print(len(train_df))\n",
    "print(len(valid_df))\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to provide useful functions and prepare the data for NN use \n",
    "class APN_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Uploads images from Dataset and set them up for Neural Network use.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        Dataset (pd.DataFrame): the DataFrame containing the dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        \"\"\"\n",
    "        Initializes the class with the DataFrame in input.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            df (pd.DataFrame): the DataFrame containing the dataset.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Calculates the number of tuples in the dataset.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            (int): number of tuples in the dataset.\n",
    "        \"\"\"    \n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves the images at the given row index.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            idx (int): dataset row index.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            A_img, P_img, N_img (ndarray): the anchor, positive and negative images at row[idx] \n",
    "        \"\"\"\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        A_img = io.imread(DATA_DIR + row.anchor)\n",
    "        P_img = io.imread(DATA_DIR + row.positive)\n",
    "        N_img = io.imread(DATA_DIR + row.negative)\n",
    "\n",
    "        #Permute because the third channel has to be in first channel in torch\n",
    "\n",
    "        #A_img = torch.from_numpy(A_img).permute(2, 0, 1) / 255.0\n",
    "        #P_img = torch.from_numpy(P_img).permute(2, 0, 1) / 255.0\n",
    "        #N_img = torch.from_numpy(N_img).permute(2, 0, 1) / 255.0\n",
    "\n",
    "\n",
    "        A_img = np.expand_dims(A_img, 0)\n",
    "        P_img = np.expand_dims(P_img, 0)\n",
    "        N_img = np.expand_dims(N_img, 0)\n",
    "\n",
    "        A_img = torch.from_numpy(A_img) / 255.0\n",
    "        P_img = torch.from_numpy(P_img) / 255.0\n",
    "        N_img = torch.from_numpy(N_img) / 255.0\n",
    "\n",
    "        #A_img = torch.from_numpy(A_img.astype(np.int32)) / 65536.0\n",
    "        #P_img = torch.from_numpy(P_img.astype(np.int32)) / 65536.0\n",
    "        #N_img = torch.from_numpy(N_img.astype(np.int32)) / 65536.0\n",
    "\n",
    "        return A_img, P_img, N_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of trainset: 16000\n",
      "Size of validset: 4000\n"
     ]
    }
   ],
   "source": [
    "# APN_Dataset objects creation\n",
    "\n",
    "trainset = APN_Dataset(train_df)\n",
    "validset = APN_Dataset(valid_df)\n",
    "\n",
    "print(f\"Size of trainset: {len(trainset)}\")\n",
    "print(f\"Size of validset: {len(validset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of batches in trainloader : 500\n",
      "No. of batches in validloader : 125\n"
     ]
    }
   ],
   "source": [
    "# DataLoader creation to handle loading data into memory\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size = BATCH_SIZE, shuffle = True)\n",
    "validloader = DataLoader(validset, batch_size = BATCH_SIZE)\n",
    "\n",
    "print(f\"No. of batches in trainloader : {len(trainloader)}\")\n",
    "print(f\"No. of batches in validloader : {len(validloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class APN_Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Defines a neural network model class APN_Model that uses an EfficientNet (specifically the B0 version) as its backbone.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, emb_size = 512):\n",
    "        \"\"\"\n",
    "        Initializes the APN_Model with a specific model and a classifier that outputs embedding vector of the specified size.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        emb_size (int, optional): the size of the output embedding vector (default is 512).\n",
    "        \"\"\"\n",
    "        super(APN_Model, self).__init__()\n",
    "\n",
    "        # Define the model to use \n",
    "        self.efficientnet = timm.create_model('tf_efficientnetv2_b0', pretrained = False)\n",
    "        \n",
    "        # Replace the classifier layer with a linear layer that outputs embeddings of size `emb_size`\n",
    "        self.efficientnet.classifier = nn.Linear(in_features=self.efficientnet.classifier.in_features, out_features = emb_size)\n",
    "\n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        Performs the forward pass of the model, which takes a batch of images and returns their embeddings.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            images (torch.Tensor): a batch of images to process.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            embeddings (torch.Tensor): a batch of embeddings of size `emb_size`.\n",
    "        \"\"\"\n",
    "        embeddings = self.efficientnet(images)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "APN_Model(\n",
       "  (efficientnet): EfficientNet(\n",
       "    (conv_stem): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNormAct2d(\n",
       "      32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "      (drop): Identity()\n",
       "      (act): SiLU(inplace=True)\n",
       "    )\n",
       "    (blocks): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): ConvBnAct(\n",
       "          (conv): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): EdgeResidual(\n",
       "          (conv_exp): Conv2dSame(16, 64, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): EdgeResidual(\n",
       "          (conv_exp): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): EdgeResidual(\n",
       "          (conv_exp): Conv2dSame(32, 128, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(128, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): EdgeResidual(\n",
       "          (conv_exp): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (conv_dw): Conv2dSame(192, 192, kernel_size=(3, 3), stride=(2, 2), groups=192, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(192, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(12, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(384, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(24, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): InvertedResidual(\n",
       "          (conv_pw): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(384, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(24, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            576, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (conv_dw): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            576, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(576, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (conv_dw): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): InvertedResidual(\n",
       "          (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (conv_dw): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (3): InvertedResidual(\n",
       "          (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (conv_dw): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (4): InvertedResidual(\n",
       "          (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (conv_dw): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (conv_dw): Conv2dSame(672, 672, kernel_size=(3, 3), stride=(2, 2), groups=672, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (conv_dw): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): InvertedResidual(\n",
       "          (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (conv_dw): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (3): InvertedResidual(\n",
       "          (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (conv_dw): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (4): InvertedResidual(\n",
       "          (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (conv_dw): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (5): InvertedResidual(\n",
       "          (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (conv_dw): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (6): InvertedResidual(\n",
       "          (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (conv_dw): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (7): InvertedResidual(\n",
       "          (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (conv_dw): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (conv_head): Conv2d(192, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (bn2): BatchNormAct2d(\n",
       "      1280, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "      (drop): Identity()\n",
       "      (act): SiLU(inplace=True)\n",
       "    )\n",
       "    (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
       "    (classifier): Linear(in_features=1280, out_features=512, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NN mod to accept greyscale fourier spectrum images instead of RGB\n",
    "\n",
    "model = APN_Model()\n",
    "model.efficientnet.conv_stem = nn.Conv2d(1, 32, 3, 2, 1, bias=False)\n",
    "\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model, dataloader, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Trains the model for one epoch using the provided dataloader, optimizer, and criterion.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        model (torch.nn.Module): the neural network model to be trained.\n",
    "        dataloader (torch.utils.data.DataLoader): the dataloader providing batches of anchor, positive, and negative image triplets.\n",
    "        optimizer (torch.optim.Optimizer): the optimizer used to update the model's parameters.\n",
    "        criterion (function): the loss function used to compute the loss between the model's outputs.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        (float) : the average training loss over all batches.\n",
    "    \"\"\"\n",
    "\n",
    "    # Sets the model to training mode\n",
    "    model.train() #ON Dropout\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Iterates over the batches of data provided by the dataloader.\n",
    "    # Each batch consists of triplets: anchor (A), positive (P), and negative (N) images.\n",
    "    for A, P, N in tqdm(dataloader):\n",
    "        # Moves the anchor, positive, and negative images to the specified device.\n",
    "        A, P, N = A.to(DEVICE), P.to(DEVICE), N.to(DEVICE)\n",
    "\n",
    "        # Passes the anchor, positive, and negative images through the model to obtain their embeddings.\n",
    "        A_embs = model(A)\n",
    "        P_embs = model(P)\n",
    "        N_embs = model(N)\n",
    "\n",
    "        # Computes the loss using the specified loss function\n",
    "        loss = criterion(A_embs, P_embs, N_embs)\n",
    "\n",
    "        # Clears the gradients of all optimized parameters.\n",
    "        # Important to prevent accumulation of gradients from multiple forward passes.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Computes the gradient of the loss with respect to the model's parameters (backpropagation).\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updates the model's parameters based on the computed gradients.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Updates toal loss at the end of each iteration\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_fn(model, dataloader, criterion):\n",
    "    \"\"\"\n",
    "    Evaluates the model using the provided dataloader and criterion.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        model (torch.nn.Module): the neural network model to be evaluated.\n",
    "        dataloader (torch.utils.data.DataLoader): the dataloader providing batches of anchor, positive, and negative image triplets.\n",
    "        criterion (function): the loss function used to compute the loss between the model's outputs.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        (float): the average validation loss over all batches.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sets the model to evaluation mode\n",
    "    model.eval() #OFF Dropout\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Disables gradient calculation.\n",
    "    # Important to reduce memory usage and speed up computations since gradients are not needed.\n",
    "    with torch.no_grad():\n",
    "        # Iterates over the batches of data provided by the dataloader.\n",
    "        for A, P, N in tqdm(dataloader):\n",
    "             # Moves the anchor, positive, and negative images to the specified device.\n",
    "            A, P, N = A.to(DEVICE), P.to(DEVICE), N.to(DEVICE)\n",
    "\n",
    "            # Passes the anchor, positive, and negative images through the model to obtain their embeddings.\n",
    "            A_embs = model(A)\n",
    "            P_embs = model(P)\n",
    "            N_embs = model(N)\n",
    "\n",
    "            # Computes the loss using the specified loss function\n",
    "            loss = criterion(A_embs, P_embs, N_embs)\n",
    "\n",
    "            # Updates toal loss at the end of each iteration\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the loss function and the optimizer to utilize.\n",
    "\n",
    "criterion = nn.TripletMarginLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LR)\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = DATA_DIR + 'best_model.pt'\n",
    "loss_path =  DATA_DIR + 'best_valid_loss.txt'\n",
    "\n",
    "def save_checkpoint(best_valid_loss):\n",
    "    \"\"\"\n",
    "    Saves best model along with its valid loss to a '.txt' file\n",
    "\n",
    "    Parametres\n",
    "    ----------\n",
    "        best_valid_loss (float): the best valid loss calculated among all training sessions.\n",
    "    \"\"\"\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    with open(loss_path, 'w') as f:\n",
    "        f.write(str(best_valid_loss))\n",
    "\n",
    "    print(\"CHECKPOINT_SAVED\")\n",
    "\n",
    "def load_checkpoint():\n",
    "    \"\"\"\n",
    "    Loads the best model and its valid loss from a '.txt' file. In case one of these can't be found, the training starts from scratch\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        best_valid_loss (float, np.Inf): if the best valid loss exists, returns a float; else returns an Infinite value\n",
    "    \"\"\"\n",
    "    if os.path.exists(model_path) and os.path.exists(loss_path):\n",
    "        print(\"CHECKPOINT_LOADED\")\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        with open(loss_path, 'r') as f:\n",
    "            best_valid_loss = float(f.read())\n",
    "            return best_valid_loss\n",
    "\n",
    "    print(\"UNABLE_TO_LOAD_CHECKPOINT\")\n",
    "    return np.Inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the best validation loss from a previously saved checkpoint.\n",
    "# (This allows the training process to resume from the best state observed so far)\n",
    "best_valid_loss = load_checkpoint()\n",
    "\n",
    "# Loops over the specified number of epochs.\n",
    "for i in range(EPOCHS):\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Trains and evaluates the model\n",
    "    train_loss = train_fn(model, trainloader, optimizer, criterion)\n",
    "    valid_loss = eval_fn(model, validloader, criterion)\n",
    "\n",
    "    # LR Scheduler updates \n",
    "    scheduler.step(valid_loss)\n",
    "\n",
    "    # If the validation loss has improved, it saves the current state of the model and updates the best valid loss\n",
    "    if valid_loss < best_valid_loss:\n",
    "        save_checkpoint(valid_loss)\n",
    "        best_valid_loss = valid_loss\n",
    "\n",
    "    # Prints the stats relative to the current epoch.\n",
    "    print(f\"EPOCHS : {i+1} train_loss : {train_loss} valid_loss : {valid_loss} Learning Rate: {scheduler.get_last_lr()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#QUESTA E' LA FUNZIONE PER GENERARE I VETTORI DI ENCODING\n",
    "def get_encoding_csv(model, anc_img_names, dirFolder):\n",
    "    \"\"\"\n",
    "    Generates encoding vectors for a set of input images using a provided model and returns a DataFrame\n",
    "    with image names and their corresponding encodings.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        model (torch.nn.Module): the neural network model used to generate embeddings.\n",
    "        anc_img_names (list or numpy.ndarray): filenames (or paths) of the anchor images.\n",
    "        dirFolder (str): the directory path where the anchor images are located.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        df_enc (pandas.DataFrame): a DataFrame containing the anchor image names and their corresponding encodings.\n",
    "    \"\"\"\n",
    "    # Convert the list of anchor image names to a NumPy array\n",
    "    anc_img_names_arr = np.array(anc_img_names)\n",
    "\n",
    "    encodings = []\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(anc_img_names_arr):\n",
    "            # Read the image from the specified directory\n",
    "            A = io.imread(dirFolder + i)\n",
    "            # Add a batch dimension to the image\n",
    "            A = np.expand_dims(A, 0)\n",
    "            # Convert the image to a PyTorch tensor and normalize to the range [0, 1]\n",
    "            A = torch.from_numpy(A.astype(np.int32)) / 255.0\n",
    "            #A = torch.from_numpy(A).permute(2, 0, 1) / 255.0 #FOR RGB IMAGES\n",
    "            \n",
    "            A = A.to(DEVICE)\n",
    "            # Pass the preprocessed image through the model to obtain its embedding\n",
    "            A_enc = model(A.unsqueeze(0))\n",
    "             # Append the embedding to the list (move to CPU and convert to NumPy array)\n",
    "            encodings.append(A_enc.squeeze().cpu().detach().numpy())\n",
    "\n",
    "        # Convert the list of embeddings to a NumPy array\n",
    "        encodings = np.array(encodings)\n",
    "        # Create a DataFrame with the embeddings\n",
    "        encodings = pd.DataFrame(encodings)\n",
    "        # Concatenate the anchor image names with their corresponding embeddings\n",
    "        df_enc = pd.concat([anc_img_names, encodings], axis = 1)\n",
    "\n",
    "        return df_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c679e667aa504a39997ef65ee046eb7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anchor</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>502</th>\n",
       "      <th>503</th>\n",
       "      <th>504</th>\n",
       "      <th>505</th>\n",
       "      <th>506</th>\n",
       "      <th>507</th>\n",
       "      <th>508</th>\n",
       "      <th>509</th>\n",
       "      <th>510</th>\n",
       "      <th>511</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fourier/fourier/afhq+afhq+afhq_v2+train+cat+re...</td>\n",
       "      <td>-0.131423</td>\n",
       "      <td>0.785294</td>\n",
       "      <td>0.060706</td>\n",
       "      <td>-0.580380</td>\n",
       "      <td>-0.412274</td>\n",
       "      <td>-0.344057</td>\n",
       "      <td>1.158957</td>\n",
       "      <td>-0.659488</td>\n",
       "      <td>-0.352364</td>\n",
       "      <td>...</td>\n",
       "      <td>1.023404</td>\n",
       "      <td>-0.525189</td>\n",
       "      <td>-0.762841</td>\n",
       "      <td>-1.152617</td>\n",
       "      <td>0.944535</td>\n",
       "      <td>-0.770070</td>\n",
       "      <td>-0.897782</td>\n",
       "      <td>-0.732184</td>\n",
       "      <td>-0.695271</td>\n",
       "      <td>-0.956114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fourier/fourier/afhq+afhq+afhq+afhq+train+cat+...</td>\n",
       "      <td>-0.131455</td>\n",
       "      <td>0.798150</td>\n",
       "      <td>0.043159</td>\n",
       "      <td>-0.624193</td>\n",
       "      <td>-0.428530</td>\n",
       "      <td>-0.327609</td>\n",
       "      <td>1.203025</td>\n",
       "      <td>-0.646166</td>\n",
       "      <td>-0.360855</td>\n",
       "      <td>...</td>\n",
       "      <td>1.036683</td>\n",
       "      <td>-0.527224</td>\n",
       "      <td>-0.755830</td>\n",
       "      <td>-1.191425</td>\n",
       "      <td>0.959072</td>\n",
       "      <td>-0.786781</td>\n",
       "      <td>-0.924531</td>\n",
       "      <td>-0.709556</td>\n",
       "      <td>-0.710499</td>\n",
       "      <td>-1.010114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fourier/fourier/afhq+afhq+afhq_v2+train+wild+r...</td>\n",
       "      <td>-0.117395</td>\n",
       "      <td>0.754934</td>\n",
       "      <td>0.070611</td>\n",
       "      <td>-0.552618</td>\n",
       "      <td>-0.399448</td>\n",
       "      <td>-0.342187</td>\n",
       "      <td>1.092596</td>\n",
       "      <td>-0.667798</td>\n",
       "      <td>-0.344100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992252</td>\n",
       "      <td>-0.509468</td>\n",
       "      <td>-0.753006</td>\n",
       "      <td>-1.100168</td>\n",
       "      <td>0.921873</td>\n",
       "      <td>-0.745311</td>\n",
       "      <td>-0.858460</td>\n",
       "      <td>-0.723608</td>\n",
       "      <td>-0.677527</td>\n",
       "      <td>-0.898834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fourier/fourier/afhq+afhq+afhq+afhq+val+dog+re...</td>\n",
       "      <td>-0.110870</td>\n",
       "      <td>0.742435</td>\n",
       "      <td>0.085500</td>\n",
       "      <td>-0.518526</td>\n",
       "      <td>-0.380408</td>\n",
       "      <td>-0.348681</td>\n",
       "      <td>1.049891</td>\n",
       "      <td>-0.665705</td>\n",
       "      <td>-0.347465</td>\n",
       "      <td>...</td>\n",
       "      <td>0.972683</td>\n",
       "      <td>-0.493763</td>\n",
       "      <td>-0.748712</td>\n",
       "      <td>-1.060761</td>\n",
       "      <td>0.912933</td>\n",
       "      <td>-0.740624</td>\n",
       "      <td>-0.831389</td>\n",
       "      <td>-0.723221</td>\n",
       "      <td>-0.662515</td>\n",
       "      <td>-0.855549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fourier/fourier/afhq+afhq+afhq_v2+train+wild+r...</td>\n",
       "      <td>-0.128772</td>\n",
       "      <td>0.775257</td>\n",
       "      <td>0.058872</td>\n",
       "      <td>-0.579144</td>\n",
       "      <td>-0.408631</td>\n",
       "      <td>-0.346341</td>\n",
       "      <td>1.159614</td>\n",
       "      <td>-0.663555</td>\n",
       "      <td>-0.351729</td>\n",
       "      <td>...</td>\n",
       "      <td>1.017458</td>\n",
       "      <td>-0.526031</td>\n",
       "      <td>-0.754862</td>\n",
       "      <td>-1.148754</td>\n",
       "      <td>0.944541</td>\n",
       "      <td>-0.769738</td>\n",
       "      <td>-0.897275</td>\n",
       "      <td>-0.724796</td>\n",
       "      <td>-0.696444</td>\n",
       "      <td>-0.953505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 513 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              anchor         0         1  \\\n",
       "0  fourier/fourier/afhq+afhq+afhq_v2+train+cat+re... -0.131423  0.785294   \n",
       "1  fourier/fourier/afhq+afhq+afhq+afhq+train+cat+... -0.131455  0.798150   \n",
       "2  fourier/fourier/afhq+afhq+afhq_v2+train+wild+r... -0.117395  0.754934   \n",
       "3  fourier/fourier/afhq+afhq+afhq+afhq+val+dog+re... -0.110870  0.742435   \n",
       "4  fourier/fourier/afhq+afhq+afhq_v2+train+wild+r... -0.128772  0.775257   \n",
       "\n",
       "          2         3         4         5         6         7         8  ...  \\\n",
       "0  0.060706 -0.580380 -0.412274 -0.344057  1.158957 -0.659488 -0.352364  ...   \n",
       "1  0.043159 -0.624193 -0.428530 -0.327609  1.203025 -0.646166 -0.360855  ...   \n",
       "2  0.070611 -0.552618 -0.399448 -0.342187  1.092596 -0.667798 -0.344100  ...   \n",
       "3  0.085500 -0.518526 -0.380408 -0.348681  1.049891 -0.665705 -0.347465  ...   \n",
       "4  0.058872 -0.579144 -0.408631 -0.346341  1.159614 -0.663555 -0.351729  ...   \n",
       "\n",
       "        502       503       504       505       506       507       508  \\\n",
       "0  1.023404 -0.525189 -0.762841 -1.152617  0.944535 -0.770070 -0.897782   \n",
       "1  1.036683 -0.527224 -0.755830 -1.191425  0.959072 -0.786781 -0.924531   \n",
       "2  0.992252 -0.509468 -0.753006 -1.100168  0.921873 -0.745311 -0.858460   \n",
       "3  0.972683 -0.493763 -0.748712 -1.060761  0.912933 -0.740624 -0.831389   \n",
       "4  1.017458 -0.526031 -0.754862 -1.148754  0.944541 -0.769738 -0.897275   \n",
       "\n",
       "        509       510       511  \n",
       "0 -0.732184 -0.695271 -0.956114  \n",
       "1 -0.709556 -0.710499 -1.010114  \n",
       "2 -0.723608 -0.677527 -0.898834  \n",
       "3 -0.723221 -0.662515 -0.855549  \n",
       "4 -0.724796 -0.696444 -0.953505  \n",
       "\n",
       "[5 rows x 513 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the best model found during training\n",
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "\n",
    "# Call 'get_encoding_csv' on the dataset to generate the DataFrame containing anchors and their encoding vector\n",
    "df_enc = get_encoding_csv(model, df['anchor'], DATA_DIR)\n",
    "\n",
    "# Save the encodings database to avoid redoing this process\n",
    "# (This process has to be redone only when finding a better model)\n",
    "df_enc.to_csv(DATA_DIR + 'database.csv', index = False)\n",
    "\n",
    "df_enc = pd.read_csv(DATA_DIR + 'database.csv')\n",
    "df_enc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_dist(img_enc, anc_enc_arr):\n",
    "    \"\"\"\n",
    "    Computes the Euclidean distance between a given image encoding and an array of anchor encodings.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        img_enc (numpy.ndarray): a 1D array representing the encoding of the image.\n",
    "        anc_enc_arr (numpy.ndarray): a 2D array representing the anchor images and their encodings.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        dist (numpy.ndarray): a 1D array containing the Euclidean distances between the image encoding and each of the anchors' one.\n",
    "    \"\"\"\n",
    "    #dist = np.sqrt(np.dot(img_enc-anc_enc_arr, (img_enc- anc_enc_arr).T))\n",
    "    dist = np.dot(img_enc-anc_enc_arr, (img_enc- anc_enc_arr).T)\n",
    "    #dist = np.sqrt(dist)\n",
    "    \n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       fourier\\fourier\\afhq+afhq+afhq+afhq+train+cat+...\n",
      "1       fourier\\fourier\\afhq+afhq+afhq+afhq+train+wild...\n",
      "2       fourier\\fourier\\afhq+afhq+afhq+afhq+train+cat+...\n",
      "3       fourier\\fourier\\afhq+afhq+afhq+afhq+train+wild...\n",
      "4       fourier\\fourier\\afhq+afhq+afhq+afhq+train+dog+...\n",
      "                              ...                        \n",
      "2495    fourier\\fourier\\imagenet+imagenet+train+n09332...\n",
      "2496    fourier\\fourier\\imagenet+imagenet+train+n04136...\n",
      "2497    fourier\\fourier\\imagenet+imagenet+train+n03868...\n",
      "2498    fourier\\fourier\\imagenet+imagenet+val+n0184306...\n",
      "2499    fourier\\fourier\\imagenet+imagenet+train+n07749...\n",
      "Name: real, Length: 2500, dtype: object\n",
      "5000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>real</th>\n",
       "      <th>fake</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fourier\\fourier\\afhq+afhq+afhq+afhq+train+cat+...</td>\n",
       "      <td>fourier\\fourier\\big_gan+big+biggan-data+905+fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fourier\\fourier\\afhq+afhq+afhq+afhq+train+wild...</td>\n",
       "      <td>fourier\\fourier\\big_gan+big+biggan-data+603+fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fourier\\fourier\\afhq+afhq+afhq+afhq+train+cat+...</td>\n",
       "      <td>fourier\\fourier\\big_gan+big+biggan-data+483+fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fourier\\fourier\\afhq+afhq+afhq+afhq+train+wild...</td>\n",
       "      <td>fourier\\fourier\\big_gan+big+biggan-data+307+fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fourier\\fourier\\afhq+afhq+afhq+afhq+train+dog+...</td>\n",
       "      <td>fourier\\fourier\\big_gan+big+biggan-data+450+fa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                real  \\\n",
       "0  fourier\\fourier\\afhq+afhq+afhq+afhq+train+cat+...   \n",
       "1  fourier\\fourier\\afhq+afhq+afhq+afhq+train+wild...   \n",
       "2  fourier\\fourier\\afhq+afhq+afhq+afhq+train+cat+...   \n",
       "3  fourier\\fourier\\afhq+afhq+afhq+afhq+train+wild...   \n",
       "4  fourier\\fourier\\afhq+afhq+afhq+afhq+train+dog+...   \n",
       "\n",
       "                                                fake  \n",
       "0  fourier\\fourier\\big_gan+big+biggan-data+905+fa...  \n",
       "1  fourier\\fourier\\big_gan+big+biggan-data+603+fa...  \n",
       "2  fourier\\fourier\\big_gan+big+biggan-data+483+fa...  \n",
       "3  fourier\\fourier\\big_gan+big+biggan-data+307+fa...  \n",
       "4  fourier\\fourier\\big_gan+big+biggan-data+450+fa...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(DATA_DIR + 'testSet.csv')\n",
    "print(test_df['real'])\n",
    "print(test_df.size)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImageEmbeddings(img, model):\n",
    "    \"\"\"\n",
    "    Generates embeddings for a given image using the provided model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        img (numpy.ndarray): the input image as a NumPy array.\n",
    "        model (torch.nn.Module): the PyTorch model used to generate the image embeddings.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        img_enc (numpy.ndarray): the embeddings of the input image.\n",
    "    \"\"\"\n",
    "    # Add a new dimension to the image array to match the expected input shape of the model\n",
    "    img = np.expand_dims(img, 0)\n",
    "    \n",
    "    # Convert the NumPy array to a PyTorch tensor and normalize pixel values to the range [0, 1]\n",
    "    img = torch.from_numpy(img) / 255.0\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Disable gradient calculation for efficiency\n",
    "    with torch.no_grad():\n",
    "        # Move the image tensor to the appropriate device (CPU or GPU)\n",
    "        img = img.to(DEVICE)\n",
    "        \n",
    "        # Add a batch dimension, pass the image through the model to get the embeddings\n",
    "        img_enc = model(img.unsqueeze(0))\n",
    "        \n",
    "        # Detach the embeddings from the computation graph and move them back to the CPU\n",
    "        img_enc = img_enc.detach().cpu().numpy()\n",
    "        \n",
    "        # Convert the embeddings to a NumPy array\n",
    "        img_enc = np.array(img_enc)\n",
    "\n",
    "    return img_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchInDatabase(img_enc, database):\n",
    "    \"\"\"\n",
    "    Searches for the closest match to a given image embedding in a database of embeddings.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img_enc (numpy.ndarray): the embedding of the input image.\n",
    "    database (pandas.DataFrame): a DataFrame containing image embeddings with 'anchor' column for image names and subsequent columns for embedding values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (str): the name of the anchor image in the database that is closest to the input image embedding.\n",
    "    \"\"\"\n",
    "    # Extract the embeddings and image names from the database\n",
    "    anc_enc_arr = database.iloc[:, 1:].to_numpy()  # All columns except the first one (assuming embeddings start from the second column)\n",
    "\n",
    "    # Initialize an empty list to store distances\n",
    "    distance = []\n",
    "\n",
    "    # Iterate over each embedding in the database\n",
    "    for i in range(anc_enc_arr.shape[0]):\n",
    "        # Compute the Euclidean distance between the input embedding and the current database embedding\n",
    "        dist = euclidean_dist(img_enc, anc_enc_arr[i : i+1, :])\n",
    "        # Append the computed distance to the distance list\n",
    "        distance = np.append(distance, dist)\n",
    "\n",
    "    # Find the index of the closest embedding by sorting distances\n",
    "    closest_idx = np.argsort(distance)\n",
    "\n",
    "    # Return the name of the closest anchor image\n",
    "    return database['anchor'][closest_idx[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_fn(test_df, currentTest, y_pred, model, database):\n",
    "    \"\"\"\n",
    "    Tests model predictive capability.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        test_df (pandas.DataFrame): DataFrame containing the test images.\n",
    "        currentTest (str): desired images to test the model on.\n",
    "        y_pred (list): a list to store the predictions ('real' or 'fake') for each test image.\n",
    "        model (torch.nn.Module): the PyTorch model used to generate the image embeddings.\n",
    "        database (pandas.DataFrame): the DataFrame containing image embeddings with 'Anchor' column for image names and subsequent columns for embedding values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Iterate over each row in the test DataFrame with progress bar\n",
    "    for index, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "        # Construct the full image path\n",
    "        img_name = DATA_DIR + row[currentTest]\n",
    "        \n",
    "        # Read the image from the path\n",
    "        img = io.imread(img_name)\n",
    "\n",
    "        # Get the image embeddings using the model\n",
    "        img_enc = getImageEmbeddings(img, model)\n",
    "\n",
    "        # Search for the closest match in the database\n",
    "        closestLabel = searchInDatabase(img_enc, database)\n",
    "\n",
    "        # Determine if the closest match is 'real' or 'fake' and append the result to pred_vector\n",
    "        if \"real\" in closestLabel:\n",
    "            y_pred.append(\"real\")\n",
    "        else:\n",
    "            y_pred.append(\"fake\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(test_df, y_pred, y_true):\n",
    "    \n",
    "    # Ground truth vectors creation\n",
    "    y_true = np.array(['fake'] * len(test_df))\n",
    "\n",
    "    temp = np.array(['real'] * len(test_df))\n",
    "\n",
    "    y_true = np.concatenate([y_true, temp])\n",
    "\n",
    "    # Confusion Matrix calculation\n",
    "    confusion_matrix(y_true, y_pred, labels=[\"real\", \"fake\"])\n",
    "\n",
    "    # True Negative, False Positive, False Negative, True Positive extraction\n",
    "    TN, FP, FN, TP = confusion_matrix(y_true, y_pred, labels=[\"real\", \"fake\"]).ravel()\n",
    "\n",
    "    # Metrics calculation\n",
    "    accuracy = round((TP + TN) /(TP + TN + FP + FN), 4) * 100\n",
    "    precision = round((TP) / (TP + FP), 4) * 100\n",
    "    sensitivy_recall = round((TP) / (TP + FN), 4) * 100\n",
    "    specificity = round((TN) / (TN + FP) * 100, 4)\n",
    "    F1_score = round((2* precision * sensitivy_recall) / (precision + sensitivy_recall), 2)\n",
    "\n",
    "    print({\"Accuracy\":accuracy,\"Precision\":precision,\"Sensitivity_recall\":sensitivy_recall, \"Specificity\": specificity, \"F1_score\":F1_score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4062425fb477444cafad0320a475789e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5af30be522c34df288d0e1f633f3b3ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': 54.64, 'Precision': 54.03, 'Sensitivity_recall': 62.239999999999995, 'Specificity': 47.04, 'F1_score': 57.85}\n"
     ]
    }
   ],
   "source": [
    "# Testing prep\n",
    "y_true = []\n",
    "y_pred = []\n",
    "tempDf = test_df\n",
    "database = df_enc\n",
    "\n",
    "\n",
    "# Testing on FAKE images\n",
    "prediction_fn(tempDf, 'fake', y_pred, model, database)\n",
    "\n",
    "# Testing on REAL images\n",
    "prediction_fn(tempDf, 'real', y_pred, model, database)\n",
    "\n",
    "# Metrics calculation\n",
    "calculate_metrics(tempDf, y_pred, y_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_testSet_dirs(test_df):\n",
    "\n",
    "\treal_dirs_list = []\n",
    "\tfake_dirs_list = []\n",
    "\n",
    "\tfor index, row in tqdm(test_df.iterrows(), total=test_df.shape[0], desc=\"Extracting generator directories\"):\n",
    "\t\t\n",
    "\t\tr_dir = row[\"real\"].split('fourier\\\\fourier\\\\')[1].split('+')[0]\n",
    "\t\tf_dir = row[\"fake\"].split('fourier\\\\fourier\\\\')[1].split('+')[0]\n",
    "\n",
    "\t\treal_dirs_list.append(r_dir)\n",
    "\t\tfake_dirs_list.append(f_dir)\n",
    "\n",
    "\treal_dirs = sorted(set(real_dirs_list))\n",
    "\tfake_dirs = sorted(set(fake_dirs_list))\n",
    "\n",
    "\treturn real_dirs, fake_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_testSet_subsets(test_df):\n",
    "\n",
    "    real_dirs, fake_dirs = get_testSet_dirs(test_df)\n",
    "    print(real_dirs)\n",
    "    print(fake_dirs)\n",
    "\n",
    "    real_subsets = {dir: test_df[test_df['real'].str.startswith(\"fourier\\\\fourier\\\\\" + dir + '+')]['real'].tolist() for dir in real_dirs}\n",
    "    fake_subsets = {dir: test_df[test_df['fake'].str.startswith(\"fourier\\\\fourier\\\\\" + dir + '+')]['fake'].tolist() for dir in fake_dirs}\n",
    "\n",
    "    real_subsets_df = {dir: pd.DataFrame(subset, columns=['real']) for dir, subset in real_subsets.items()}\n",
    "    fake_subsets_df = {dir: pd.DataFrame(subset, columns=['fake']) for dir, subset in fake_subsets.items()}\n",
    "\n",
    "    return real_subsets_df, fake_subsets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65d58b30f82b49b091f08e7c37fb83d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting generator directories:   0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['afhq', 'celebahq', 'coco', 'ffhq', 'imagenet']\n",
      "['big_gan', 'denoising_diffusion_gan', 'diffusion_gan', 'gansformer', 'gau_gan', 'projected_gan', 'star_gan', 'stylegan1', 'stylegan2', 'stylegan3']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "910d6e576bc04c17b1619822a5bf18bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Metrics summary for big_gan:\n",
      "lunghezza di y_pred prima : 250\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bec5d92a8ec34d12af54178b91420f39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lunghezza di y_pred dopo: 500\n",
      "{'Accuracy': 38.6, 'Precision': 41.010000000000005, 'Sensitivity_recall': 52.0, 'Specificity': 25.2, 'F1_score': 45.86}\n",
      "\n",
      "\n",
      "\n",
      "Metrics summary for denoising_diffusion_gan:\n",
      "lunghezza di y_pred prima : 250\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f49e0e4f2104b31970f42fc942eacb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lunghezza di y_pred dopo: 500\n",
      "{'Accuracy': 45.0, 'Precision': 45.61, 'Sensitivity_recall': 52.0, 'Specificity': 38.0, 'F1_score': 48.6}\n",
      "\n",
      "\n",
      "\n",
      "Metrics summary for diffusion_gan:\n",
      "lunghezza di y_pred prima : 250\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5e1333e239f4cd287b5b2a6170b7f67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lunghezza di y_pred dopo: 500\n",
      "{'Accuracy': 48.0, 'Precision': 48.15, 'Sensitivity_recall': 52.0, 'Specificity': 44.0, 'F1_score': 50.0}\n",
      "\n",
      "\n",
      "\n",
      "Metrics summary for gansformer:\n",
      "lunghezza di y_pred prima : 250\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5418da7bdf8e47ec85b8e5d6160ffa81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lunghezza di y_pred dopo: 500\n",
      "{'Accuracy': 41.6, 'Precision': 43.05, 'Sensitivity_recall': 52.0, 'Specificity': 31.2, 'F1_score': 47.1}\n",
      "\n",
      "\n",
      "\n",
      "Metrics summary for gau_gan:\n",
      "lunghezza di y_pred prima : 250\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed53b434b7a445fbbe539a1aa22e33a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lunghezza di y_pred dopo: 500\n",
      "{'Accuracy': 47.199999999999996, 'Precision': 47.449999999999996, 'Sensitivity_recall': 52.0, 'Specificity': 42.4, 'F1_score': 49.62}\n",
      "\n",
      "\n",
      "\n",
      "Metrics summary for projected_gan:\n",
      "lunghezza di y_pred prima : 250\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b3ef9f0c6ad41849d53f26699d59676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lunghezza di y_pred dopo: 500\n",
      "{'Accuracy': 45.800000000000004, 'Precision': 46.26, 'Sensitivity_recall': 52.0, 'Specificity': 39.6, 'F1_score': 48.96}\n",
      "\n",
      "\n",
      "\n",
      "Metrics summary for star_gan:\n",
      "lunghezza di y_pred prima : 250\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8628046012940c985a90fadcf56257b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lunghezza di y_pred dopo: 500\n",
      "{'Accuracy': 40.8, 'Precision': 42.480000000000004, 'Sensitivity_recall': 52.0, 'Specificity': 29.6, 'F1_score': 46.76}\n",
      "\n",
      "\n",
      "\n",
      "Metrics summary for stylegan1:\n",
      "lunghezza di y_pred prima : 250\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4b2e329122b49a98d8c05a3769512ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lunghezza di y_pred dopo: 500\n",
      "{'Accuracy': 45.4, 'Precision': 45.94, 'Sensitivity_recall': 52.0, 'Specificity': 38.8, 'F1_score': 48.78}\n",
      "\n",
      "\n",
      "\n",
      "Metrics summary for stylegan2:\n",
      "lunghezza di y_pred prima : 250\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3fd017074644c2e8ab09f75665e15bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lunghezza di y_pred dopo: 500\n",
      "{'Accuracy': 49.4, 'Precision': 49.43, 'Sensitivity_recall': 52.0, 'Specificity': 46.8, 'F1_score': 50.68}\n",
      "\n",
      "\n",
      "\n",
      "Metrics summary for stylegan3:\n",
      "lunghezza di y_pred prima : 250\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd4ab3c8357f4f5089f6d976ddfcac73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lunghezza di y_pred dopo: 500\n",
      "{'Accuracy': 47.0, 'Precision': 47.27, 'Sensitivity_recall': 52.0, 'Specificity': 42.0, 'F1_score': 49.52}\n"
     ]
    }
   ],
   "source": [
    "# HERE ARE CALCULATED INDIVIDUAL METRICS FOR EACH FAKE GENERATOR\n",
    "real_subsets, fake_subsets = get_testSet_subsets(test_df)\n",
    "\n",
    "# Lenght matching \n",
    "first_real_key = next(iter(real_subsets))\n",
    "first_fake_key = next(iter(fake_subsets))\n",
    "\n",
    "real_df = real_subsets[first_real_key]\n",
    "first_fake_df = fake_subsets[first_fake_key] \n",
    "\n",
    "real_df = real_df.head(len(first_fake_df))\n",
    "\n",
    "# Testing prep\n",
    "y_pred_real = [] #Avoid redoing predictions on the same real images subset\n",
    "y_pred = []\n",
    "database = df_enc\n",
    "\n",
    "# Testing on REAL images\n",
    "prediction_fn(real_df, 'real', y_pred_real, model, database)\n",
    "\n",
    "for dir, df in fake_subsets.items():\n",
    "    print(f\"\\n\\n\\nMetrics summary for {dir}:\")\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = y_pred_real.copy()\n",
    "\n",
    "    # Testing on FAKE images\n",
    "    prediction_fn(df, 'fake', y_pred, model, database)\n",
    "    # Metrics calculation\n",
    "    # (the df in input makes no differences)\n",
    "    calculate_metrics(df, y_pred, y_true)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepfake",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
