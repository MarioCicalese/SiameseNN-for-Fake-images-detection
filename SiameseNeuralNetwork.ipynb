{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import DatasetPath\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import timm\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from skimage import io\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = DatasetPath.effectivePath\n",
    "INPUT_CSV_FILE = DATASET_PATH + \"input.csv\"\n",
    "\n",
    "with open(DATASET_PATH + 'best_valid_loss.txt', 'w') as file:\n",
    "    file.write(str(np.inf))\n",
    "\n",
    "#ATTENZIONE, IL BATCH SIZE VA REGOLATO IN BASE ALLA DISPONIBILITA' DELLA VOSTRA MEMORIA. TENETE CONTO CHE IO HO 128 CON 6GB DI MEMORIA GPU\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "#ATTENZIONE, IL LEARNING RATE ANCHE E' UN ALTRO IPER PARAMETRO IO HO FATTO LE PROVE SONO CON QUESTO.\n",
    "LR = 0.001\n",
    "\n",
    "#ATTENZIONE, L'EPOCHE DI TRAINING E' UN ALTRO IPERPARAMETRO IO PER ORA L'HO SETTATO COSI, MA SI PUò AUMENTARE O DIMINUIRE\n",
    "EPOCHS = 5\n",
    "\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000\n",
      "4000\n",
      "(20000, 3)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(INPUT_CSV_FILE)\n",
    "\n",
    "train_df, valid_df = train_test_split(df, test_size = 0.20, random_state = 42)\n",
    "print(len(train_df))\n",
    "print(len(valid_df))\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNZIONE PER CARICARE LE IMMAGINI DAL FILE CSV E SETTARLE NEL MODO CORRETO PER ESSERE DATE COME INPUT ALLA RETE NEURALE\n",
    "class APN_Dataset(Dataset):\n",
    "\n",
    "  def __init__(self, df):\n",
    "    self.df = df\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.df)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    row = self.df.iloc[idx]\n",
    "\n",
    "    A_img = io.imread(DATASET_PATH + row.anchor)\n",
    "    P_img = io.imread(DATASET_PATH + row.positive)\n",
    "    N_img = io.imread(DATASET_PATH + row.negative)\n",
    "\n",
    "    #Permute because the third channel has to be in first channel in torch\n",
    "\n",
    "    #A_img = torch.from_numpy(A_img).permute(2, 0, 1) / 255.0\n",
    "    #P_img = torch.from_numpy(P_img).permute(2, 0, 1) / 255.0\n",
    "    #N_img = torch.from_numpy(N_img).permute(2, 0, 1) / 255.0\n",
    "\n",
    "\n",
    "    A_img = np.expand_dims(A_img, 0)\n",
    "    P_img = np.expand_dims(P_img, 0)\n",
    "    N_img = np.expand_dims(N_img, 0)\n",
    "\n",
    "    A_img = torch.from_numpy(A_img) / 255.0\n",
    "    P_img = torch.from_numpy(P_img) / 255.0\n",
    "    N_img = torch.from_numpy(N_img) / 255.0\n",
    "\n",
    "    #A_img = torch.from_numpy(A_img.astype(np.int32)) / 65536.0\n",
    "    #P_img = torch.from_numpy(P_img.astype(np.int32)) / 65536.0\n",
    "    #N_img = torch.from_numpy(N_img.astype(np.int32)) / 65536.0\n",
    "\n",
    "    return A_img, P_img, N_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of trainset: 16000\n",
      "Size of validset: 4000\n"
     ]
    }
   ],
   "source": [
    "trainset = APN_Dataset(train_df)\n",
    "validset = APN_Dataset(valid_df)\n",
    "\n",
    "print(f\"Size of trainset: {len(trainset)}\")\n",
    "print(f\"Size of validset: {len(validset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of batches in trainloader : 500\n",
      "No. of batches in validloader : 125\n"
     ]
    }
   ],
   "source": [
    "trainloader = DataLoader(trainset, batch_size = BATCH_SIZE, shuffle = True)\n",
    "validloader = DataLoader(validset, batch_size = BATCH_SIZE)\n",
    "\n",
    "print(f\"No. of batches in trainloader : {len(trainloader)}\")\n",
    "print(f\"No. of batches in validloader : {len(validloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNZIONE PER CARICARE IL MODELLO DI RETE NEURALE DIRETTAMENTE DALLE REPOSITORY ONLINE\n",
    "class APN_Model(nn.Module):\n",
    "\n",
    "    #QUI DEFINISCO LA SIZE DEL VETTORE DI EMBEDDING\n",
    "  def __init__(self, emb_size = 512):\n",
    "    super(APN_Model, self).__init__()\n",
    "\n",
    "    #QUI CAIRCATE IL MODELLO, IN QUESTO CASO EFFICIENTNET VERSIONE B0 (LA PIù LEGGERA DELLA FAMIGLIA)\n",
    "    self.efficientnet = timm.create_model('tf_efficientnetv2_b0', pretrained = False)\n",
    "    self.efficientnet.classifier = nn.Linear(in_features=self.efficientnet.classifier.in_features, out_features = emb_size)\n",
    "\n",
    "  def forward(self, images):\n",
    "    embeddings = self.efficientnet(images)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": null,
   "metadata": {},
   "outputs": [],

   "source": [
    "#QUI FATE UNA PICCOLA MODIFICA ALLA RETE PER FARLE AVERE IN INPUT IMMAGINI IN SCALA DI GRIGIO DELLO SPETTRO DI FOURIER\n",
    "model = APN_Model()\n",
    "model.efficientnet.conv_stem = nn.Conv2d(1, 32, 3, 2, 1, bias=False)\n",
    "\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNZIONE DI TRAINING\n",
    "def train_fn(model, dataloader, optimizer, criterion):\n",
    "  model.train() #ON Dropout\n",
    "  total_loss = 0.0\n",
    "  \n",
    "  for A, P, N in tqdm(dataloader, desc=\"Training Progress\"):\n",
    "    A, P, N = A.to(DEVICE), P.to(DEVICE), N.to(DEVICE)\n",
    "    A_embs = model(A)\n",
    "    P_embs = model(P)\n",
    "    N_embs = model(N)\n",
    "    loss = criterion(A_embs, P_embs, N_embs)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    total_loss += loss.item()\n",
    "\n",
    "  return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNZIONE DI EVALUATION\n",
    "def eval_fn(model, dataloader, criterion):\n",
    "  model.eval() #OFF Dropout\n",
    "  total_loss = 0.0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for A, P, N in tqdm(dataloader, desc = \"Validation progress\"):\n",
    "      A, P, N = A.to(DEVICE), P.to(DEVICE), N.to(DEVICE)\n",
    "\n",
    "      A_embs = model(A)\n",
    "      P_embs = model(P)\n",
    "      N_embs = model(N)\n",
    "\n",
    "      loss = criterion(A_embs, P_embs, N_embs)\n",
    "\n",
    "      total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#QUI UTILIZZATE LA TRIPLET LOSS E COME OTTIMIZZATORE ADAM. PER IL MOMENTO VI SCONSIGLIO DI CAMBIARE ADAM\n",
    "criterion = nn.TripletMarginLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LR)"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": null,
   "metadata": {},
   "outputs": [],

   "source": [
    "model_path = DATASET_PATH + 'best_model.pt'\n",
    "loss_path =  DATASET_PATH + 'best_valid_loss.txt'\n",
    "\n",
    "def save_checkpoint(best_valid_loss):\n",
    "  \"\"\"\n",
    "  Saves best model along with its valid loss to a '.txt' file\n",
    "\n",
    "  Parametres\n",
    "  ----------\n",
    "    best_valid_loss (float): the best valid loss calculated among all training sessions.\n",
    "  \"\"\"\n",
    "  torch.save(model.state_dict(), model_path)\n",
    "  with open(loss_path, 'w') as f:\n",
    "    f.write(str(best_valid_loss))\n",
    "  \n",
    "  print(\"CHECKPOINT_SAVED\")\n",
    "\n",
    "\n",
    "def load_checkpoint():\n",
    "  \"\"\"\n",
    "  Loads the best model and its valid loss from a '.txt' file. In case one of these can't be found, the training starts from scratch\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "    best_valid_loss (float, np.Inf): if the best valid loss exists, returns a float; else returns an Infinite value\n",
    "  \"\"\"\n",
    "  if os.path.exists(model_path) and os.path.exists(loss_path):\n",
    "    print(\"CHECKPOINT_LOADED\")\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    with open(loss_path, 'r') as f:\n",
    "      best_valid_loss = float(f.read())\n",
    "      return best_valid_loss\n",
    "  \n",
    "  print(\"UNABLE_TO_LOAD_CHECKPOINT\")\n",
    "  return np.Inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "611f4f1872784608b8d0e5c5a46e401d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m   best_valid_loss \u001b[38;5;241m=\u001b[39m contenuto\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[1;32m---> 11\u001b[0m   train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m   valid_loss \u001b[38;5;241m=\u001b[39m eval_fn(model, validloader, criterion)\n\u001b[0;32m     14\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m valid_loss \u001b[38;5;241m<\u001b[39m best_valid_loss:\n",
      "Cell \u001b[1;32mIn[9], line 17\u001b[0m, in \u001b[0;36mtrain_fn\u001b[1;34m(model, dataloader, optimizer, criterion)\u001b[0m\n\u001b[0;32m     14\u001b[0m   loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     15\u001b[0m   optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 17\u001b[0m   total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#QUI FATE IL TRAINING VERO E PROPRIO UTILIZZANDO TUTTE LE FUNZIONI CHE AVETE VISTO IN PRECEDENZA\n",
    "best_valid_loss = load_checkpoint()\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "\n",
    "  train_loss = train_fn(model, trainloader, optimizer, criterion)\n",
    "  valid_loss = eval_fn(model, validloader, criterion)\n",
    "\n",
    "  if valid_loss < best_valid_loss:\n",
    "    save_checkpoint(valid_loss)\n",
    "    best_valid_loss = valid_loss\n",
    "\n",
    "  print(f\"EPOCHS : {i+1} train_loss : {train_loss} valid_loss : {valid_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "318edfe0b3a748278ec4a3995eb416f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m best_valid_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mInf\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[1;32m----> 5\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     valid_loss \u001b[38;5;241m=\u001b[39m eval_fn(model, validloader, criterion)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m valid_loss \u001b[38;5;241m<\u001b[39m best_valid_loss:\n",
      "Cell \u001b[1;32mIn[9], line 14\u001b[0m, in \u001b[0;36mtrain_fn\u001b[1;34m(model, dataloader, optimizer, criterion)\u001b[0m\n\u001b[0;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(A_embs, P_embs, N_embs)\n\u001b[0;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 14\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     17\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\mario\\.conda\\envs\\deepfake\\lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mario\\.conda\\envs\\deepfake\\lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mario\\.conda\\envs\\deepfake\\lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "best_valid_loss = np.Inf\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "\n",
    "    train_loss = train_fn(model, trainloader, optimizer, criterion)\n",
    "    valid_loss = eval_fn(model, validloader, criterion)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        torch.save(model.state_dict(), DATASET_PATH + 'best_model.pt')\n",
    "        best_valid_loss = valid_loss\n",
    "        print(\"SAVED_WEIGHTS_SUCCESS\")\n",
    "\n",
    "    print(f\"EPOCHS : {i+1} train_loss : {train_loss} valid_loss : {valid_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#QUESTA E' LA FUNZIONE PER GENERARE I VETTORI DI ENCODING\n",
    "def get_encoding_csv(model, anc_img_path):\n",
    "  anc_img_names_arr = np.array(anc_img_path)\n",
    "  encodings = []\n",
    "\n",
    "  model.eval()\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for image in tqdm(anc_img_names_arr):\n",
    "      A = io.imread(DATASET_PATH + image)\n",
    "      #A = torch.from_numpy(A).permute(2, 0, 1) / 255.0\n",
    "      A = np.expand_dims(A, 0)\n",
    "      A = torch.from_numpy(A.astype(np.int32)) / 255.0\n",
    "      A = A.to(DEVICE)\n",
    "      A_enc = model(A.unsqueeze(0))\n",
    "      encodings.append(A_enc.squeeze().cpu().detach().numpy())\n",
    "\n",
    "    encodings = np.array(encodings)\n",
    "    encodings = pd.DataFrame(encodings)\n",
    "    df_enc = pd.concat([anc_img_path, encodings], axis = 1)\n",
    "\n",
    "    return df_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96ae47f969044cfa82f0636d4d2dab0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#QUI RICARICO IL MODELLO UNA VOLTA TRAINATO\n",
    "model.load_state_dict(torch.load(DATASET_PATH + 'best_model.pt'))\n",
    "\n",
    "#QUI CREO IL DATABASE DI FEATURE VECTORS DEL TRAINING SET\n",
    "df_enc = get_encoding_csv(model, df['anchor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enc.to_csv('database.csv', index = False)\n",
    "\n",
    "df_enc = pd.read_csv(DATASET_PATH + 'embedded.csv')\n",
    "df_enc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_dist(img_enc, anc_enc_arr):\n",
    "    #dist = np.s qrt(np.dot(img_enc-anc_enc_arr, (img_enc- anc_enc_arr).T))\n",
    "    dist = np.dot(img_enc-anc_enc_arr, (img_enc- anc_enc_arr).T)\n",
    "    #dist = np.sqrt(dist)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImageEmbeddings(img, model):\n",
    "\n",
    "    img = np.expand_dims(img, 0)\n",
    "    img = torch.from_numpy(img) / 255\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        img = img.to(DEVICE)\n",
    "        img_enc = model(img.unsqueeze(0))\n",
    "        img_enc = img_enc.detach().cpu().numpy()\n",
    "        img_enc = np.array(img_enc)\n",
    "\n",
    "    return img_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchInDatabase(img_enc, database):\n",
    "    anc_enc_arr = database.iloc[:, 1:].to_numpy()\n",
    "    anc_img_names = database['anchor']\n",
    "\n",
    "    distance = []\n",
    "    for i in range(anc_enc_arr.shape[0]):\n",
    "        dist = euclidean_dist(img_enc, anc_enc_arr[i : i+1, :])\n",
    "        distance = np.append(distance, dist)\n",
    "\n",
    "    closest_idx = np.argsort(distance)\n",
    "\n",
    "    return database['anchor'][closest_idx[0]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepfake",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
