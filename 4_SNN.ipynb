{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import timm\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from skimage import io\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "from constants import DatasetPath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UTILITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the Dataset path has been specified in the 'constants.py' file\n",
    "DATA_DIR = DatasetPath.effectivePath\n",
    "\n",
    "# HYPERPARAMETRES\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "LR = 0.001\n",
    "\n",
    "EPOCHS = 30\n",
    "\n",
    "# Device to run calculations on \n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_DIR + \"trainingSet.csv\")\n",
    "\n",
    "train_df, valid_df = train_test_split(df, test_size = 0.20, random_state = 42)\n",
    "print(len(train_df))\n",
    "print(len(valid_df))\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PREPARATION & LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to provide useful functions and prepare the data for NN use \n",
    "class APN_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Uploads images from Dataset and set them up for Neural Network use.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        Dataset (pd.DataFrame): the DataFrame containing the dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        \"\"\"\n",
    "        Initializes the class with the DataFrame in input.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            df (pd.DataFrame): the DataFrame containing the dataset.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Calculates the number of tuples in the dataset.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            (int): number of tuples in the dataset.\n",
    "        \"\"\"    \n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves the images at the given row index.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            idx (int): dataset row index.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            A_img, P_img, N_img (ndarray): the anchor, positive and negative images at row[idx] \n",
    "        \"\"\"\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        A_img = io.imread(DATA_DIR + row.anchor)\n",
    "        P_img = io.imread(DATA_DIR + row.positive)\n",
    "        N_img = io.imread(DATA_DIR + row.negative)\n",
    "\n",
    "        #Permute because the third channel has to be in first channel in torch\n",
    "\n",
    "        #A_img = torch.from_numpy(A_img).permute(2, 0, 1) / 255.0\n",
    "        #P_img = torch.from_numpy(P_img).permute(2, 0, 1) / 255.0\n",
    "        #N_img = torch.from_numpy(N_img).permute(2, 0, 1) / 255.0\n",
    "\n",
    "\n",
    "        A_img = np.expand_dims(A_img, 0)\n",
    "        P_img = np.expand_dims(P_img, 0)\n",
    "        N_img = np.expand_dims(N_img, 0)\n",
    "\n",
    "        A_img = torch.from_numpy(A_img) / 255.0\n",
    "        P_img = torch.from_numpy(P_img) / 255.0\n",
    "        N_img = torch.from_numpy(N_img) / 255.0\n",
    "\n",
    "        #A_img = torch.from_numpy(A_img.astype(np.int32)) / 65536.0\n",
    "        #P_img = torch.from_numpy(P_img.astype(np.int32)) / 65536.0\n",
    "        #N_img = torch.from_numpy(N_img.astype(np.int32)) / 65536.0\n",
    "\n",
    "        return A_img, P_img, N_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APN_Dataset objects creation\n",
    "\n",
    "trainset = APN_Dataset(train_df)\n",
    "validset = APN_Dataset(valid_df)\n",
    "\n",
    "print(f\"Size of trainset: {len(trainset)}\")\n",
    "print(f\"Size of validset: {len(validset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader creation to handle loading data into memory\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size = BATCH_SIZE, shuffle = True)\n",
    "validloader = DataLoader(validset, batch_size = BATCH_SIZE)\n",
    "\n",
    "print(f\"No. of batches in trainloader : {len(trainloader)}\")\n",
    "print(f\"No. of batches in validloader : {len(validloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL DEFINITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class APN_Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Defines a neural network model class APN_Model that uses an EfficientNet (specifically the B0 version) as its backbone.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, emb_size = 512):\n",
    "        \"\"\"\n",
    "        Initializes the APN_Model with a specific model and a classifier that outputs embedding vector of the specified size.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        emb_size (int, optional): the size of the output embedding vector (default is 512).\n",
    "        \"\"\"\n",
    "        super(APN_Model, self).__init__()\n",
    "\n",
    "        # Define the model to use \n",
    "        self.efficientnet = timm.create_model('tf_efficientnetv2_b0', pretrained = False)\n",
    "        \n",
    "        # Replace the classifier layer with a linear layer that outputs embeddings of size `emb_size`\n",
    "        self.efficientnet.classifier = nn.Linear(in_features=self.efficientnet.classifier.in_features, out_features = emb_size)\n",
    "\n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        Performs the forward pass of the model, which takes a batch of images and returns their embeddings.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            images (torch.Tensor): a batch of images to process.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            embeddings (torch.Tensor): a batch of embeddings of size `emb_size`.\n",
    "        \"\"\"\n",
    "        embeddings = self.efficientnet(images)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN mod to accept greyscale fourier spectrum images instead of RGB\n",
    "\n",
    "model = APN_Model()\n",
    "model.efficientnet.conv_stem = nn.Conv2d(1, 32, 3, 2, 1, bias=False)\n",
    "\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model, dataloader, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Trains the model for one epoch using the provided dataloader, optimizer, and criterion.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        model (torch.nn.Module): the neural network model to be trained.\n",
    "        dataloader (torch.utils.data.DataLoader): the dataloader providing batches of anchor, positive, and negative image triplets.\n",
    "        optimizer (torch.optim.Optimizer): the optimizer used to update the model's parameters.\n",
    "        criterion (function): the loss function used to compute the loss between the model's outputs.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        (float) : the average training loss over all batches.\n",
    "    \"\"\"\n",
    "\n",
    "    # Sets the model to training mode\n",
    "    model.train() #ON Dropout\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Iterates over the batches of data provided by the dataloader.\n",
    "    # Each batch consists of triplets: anchor (A), positive (P), and negative (N) images.\n",
    "    for A, P, N in tqdm(dataloader):\n",
    "        # Moves the anchor, positive, and negative images to the specified device.\n",
    "        A, P, N = A.to(DEVICE), P.to(DEVICE), N.to(DEVICE)\n",
    "\n",
    "        # Passes the anchor, positive, and negative images through the model to obtain their embeddings.\n",
    "        A_embs = model(A)\n",
    "        P_embs = model(P)\n",
    "        N_embs = model(N)\n",
    "\n",
    "        # Computes the loss using the specified loss function\n",
    "        loss = criterion(A_embs, P_embs, N_embs)\n",
    "\n",
    "        # Clears the gradients of all optimized parameters.\n",
    "        # Important to prevent accumulation of gradients from multiple forward passes.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Computes the gradient of the loss with respect to the model's parameters (backpropagation).\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updates the model's parameters based on the computed gradients.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Updates toal loss at the end of each iteration\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_fn(model, dataloader, criterion):\n",
    "    \"\"\"\n",
    "    Evaluates the model using the provided dataloader and criterion.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        model (torch.nn.Module): the neural network model to be evaluated.\n",
    "        dataloader (torch.utils.data.DataLoader): the dataloader providing batches of anchor, positive, and negative image triplets.\n",
    "        criterion (function): the loss function used to compute the loss between the model's outputs.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        (float): the average validation loss over all batches.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sets the model to evaluation mode\n",
    "    model.eval() #OFF Dropout\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Disables gradient calculation.\n",
    "    # Important to reduce memory usage and speed up computations since gradients are not needed.\n",
    "    with torch.no_grad():\n",
    "        # Iterates over the batches of data provided by the dataloader.\n",
    "        for A, P, N in tqdm(dataloader):\n",
    "             # Moves the anchor, positive, and negative images to the specified device.\n",
    "            A, P, N = A.to(DEVICE), P.to(DEVICE), N.to(DEVICE)\n",
    "\n",
    "            # Passes the anchor, positive, and negative images through the model to obtain their embeddings.\n",
    "            A_embs = model(A)\n",
    "            P_embs = model(P)\n",
    "            N_embs = model(N)\n",
    "\n",
    "            # Computes the loss using the specified loss function\n",
    "            loss = criterion(A_embs, P_embs, N_embs)\n",
    "\n",
    "            # Updates toal loss at the end of each iteration\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the loss function and the optimizer to utilize.\n",
    "\n",
    "criterion = nn.TripletMarginLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LR)\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(best_valid_loss):\n",
    "    \"\"\"\n",
    "    Saves best model along with its valid loss to a '.txt' file.\n",
    "\n",
    "    Files are saved in the CWD\n",
    "\n",
    "    Parametres\n",
    "    ----------\n",
    "        best_valid_loss (float): the best valid loss calculated among all training sessions.\n",
    "    \"\"\"\n",
    "    torch.save(model.state_dict(), 'best_model.pt')\n",
    "    with open('best_valid_loss.txt', 'w') as f:\n",
    "        f.write(str(best_valid_loss))\n",
    "\n",
    "    print(\"CHECKPOINT_SAVED\")\n",
    "\n",
    "def load_checkpoint():\n",
    "    \"\"\"\n",
    "    Loads the best model and its valid loss from a '.txt' file. In case one of these can't be found, the training starts from scratch\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        best_valid_loss (float, np.Inf): if the best valid loss exists, returns a float; else returns an Infinite value\n",
    "    \"\"\"\n",
    "    if os.path.exists('best_model.pt') and os.path.exists('best_valid_loss.txt'):\n",
    "        print(\"CHECKPOINT_LOADED\")\n",
    "        model.load_state_dict(torch.load('best_model.pt'))\n",
    "        with open('best_valid_loss.txt', 'r') as f:\n",
    "            best_valid_loss = float(f.read())\n",
    "            return best_valid_loss\n",
    "\n",
    "    print(\"UNABLE_TO_LOAD_CHECKPOINT\")\n",
    "    return np.Inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the best validation loss from a previously saved checkpoint.\n",
    "# (This allows the training process to resume from the best state observed so far)\n",
    "best_valid_loss = load_checkpoint()\n",
    "\n",
    "# Lists to store train_loss and valid_loss values\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "# Loops over the specified number of epochs.\n",
    "for i in range(EPOCHS):\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Trains and evaluates the model\n",
    "    train_loss = train_fn(model, trainloader, optimizer, criterion)\n",
    "    valid_loss = eval_fn(model, validloader, criterion)\n",
    "\n",
    "    # LR Scheduler updates \n",
    "    scheduler.step(valid_loss)\n",
    "\n",
    "    # If the validation loss has improved, it saves the current state of the model and updates the best valid loss\n",
    "    if valid_loss < best_valid_loss:\n",
    "        save_checkpoint(valid_loss)\n",
    "        best_valid_loss = valid_loss\n",
    "\n",
    "    # Save train_loss and valid_loss values\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "\n",
    "    # Prints the stats relative to the current epoch.\n",
    "    print(f\"EPOCHS : {i+1} train_loss : {train_loss} valid_loss : {valid_loss} Learning Rate: {scheduler.get_last_lr()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LEARNING CURVE VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot train_loss and valid_loss\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(valid_losses, label='Valid Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANCHOR EMBEDDINGS DATABASE CREATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#QUESTA E' LA FUNZIONE PER GENERARE I VETTORI DI ENCODING\n",
    "def get_encoding_csv(model, anc_img_names, dirFolder):\n",
    "    \"\"\"\n",
    "    Generates encoding vectors for a set of input images using a provided model and returns a DataFrame\n",
    "    with image names and their corresponding encodings.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        model (torch.nn.Module): the neural network model used to generate embeddings.\n",
    "        anc_img_names (list or numpy.ndarray): filenames (or paths) of the anchor images.\n",
    "        dirFolder (str): the directory path where the anchor images are located.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        df_enc (pandas.DataFrame): a DataFrame containing the anchor image names and their corresponding encodings.\n",
    "    \"\"\"\n",
    "    # Convert the list of anchor image names to a NumPy array\n",
    "    anc_img_names_arr = np.array(anc_img_names)\n",
    "\n",
    "    encodings = []\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(anc_img_names_arr):\n",
    "            # Read the image from the specified directory\n",
    "            A = io.imread(dirFolder + i)\n",
    "            # Add a batch dimension to the image\n",
    "            A = np.expand_dims(A, 0)\n",
    "            # Convert the image to a PyTorch tensor and normalize to the range [0, 1]\n",
    "            A = torch.from_numpy(A.astype(np.int32)) / 255.0\n",
    "            #A = torch.from_numpy(A).permute(2, 0, 1) / 255.0 #FOR RGB IMAGES\n",
    "            \n",
    "            A = A.to(DEVICE)\n",
    "            # Pass the preprocessed image through the model to obtain its embedding\n",
    "            A_enc = model(A.unsqueeze(0))\n",
    "             # Append the embedding to the list (move to CPU and convert to NumPy array)\n",
    "            encodings.append(A_enc.squeeze().cpu().detach().numpy())\n",
    "\n",
    "        # Convert the list of embeddings to a NumPy array\n",
    "        encodings = np.array(encodings)\n",
    "        # Create a DataFrame with the embeddings\n",
    "        encodings = pd.DataFrame(encodings)\n",
    "        # Concatenate the anchor image names with their corresponding embeddings\n",
    "        df_enc = pd.concat([anc_img_names, encodings], axis = 1)\n",
    "\n",
    "        return df_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model found during training\n",
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "\n",
    "# Call 'get_encoding_csv' on the dataset to generate the DataFrame containing anchors and their encoding vector\n",
    "df_enc = get_encoding_csv(model, df['anchor'], DATA_DIR)\n",
    "\n",
    "# Save the encodings database to avoid redoing this process\n",
    "# (This process has to be redone only when finding a better model)\n",
    "df_enc.to_csv(DATA_DIR + 'database.csv', index = False)\n",
    "df_enc.to_csv('database.csv', index = False) #Double save also in the CWD\n",
    "\n",
    "df_enc = pd.read_csv(DATA_DIR + 'database.csv')\n",
    "df_enc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UTILITY FUNCTIONS DEFINITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_dist(img_enc, anc_enc_arr):\n",
    "    \"\"\"\n",
    "    Computes the Euclidean distance between a given image encoding and an array of anchor encodings.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        img_enc (numpy.ndarray): a 1D array representing the encoding of the image.\n",
    "        anc_enc_arr (numpy.ndarray): a 2D array representing the anchor images and their encodings.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        dist (numpy.ndarray): a 1D array containing the Euclidean distances between the image encoding and each of the anchors' one.\n",
    "    \"\"\"\n",
    "    #dist = np.sqrt(np.dot(img_enc-anc_enc_arr, (img_enc- anc_enc_arr).T))\n",
    "    dist = np.dot(img_enc-anc_enc_arr, (img_enc- anc_enc_arr).T)\n",
    "    #dist = np.sqrt(dist)\n",
    "    \n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(DATA_DIR + 'testSet.csv')\n",
    "print(test_df['real'])\n",
    "print(test_df.size)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImageEmbeddings(img, model):\n",
    "    \"\"\"\n",
    "    Generates embeddings for a given image using the provided model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        img (numpy.ndarray): the input image as a NumPy array.\n",
    "        model (torch.nn.Module): the PyTorch model used to generate the image embeddings.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        img_enc (numpy.ndarray): the embeddings of the input image.\n",
    "    \"\"\"\n",
    "    # Add a new dimension to the image array to match the expected input shape of the model\n",
    "    img = np.expand_dims(img, 0)\n",
    "    \n",
    "    # Convert the NumPy array to a PyTorch tensor and normalize pixel values to the range [0, 1]\n",
    "    img = torch.from_numpy(img) / 255.0\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Disable gradient calculation for efficiency\n",
    "    with torch.no_grad():\n",
    "        # Move the image tensor to the appropriate device (CPU or GPU)\n",
    "        img = img.to(DEVICE)\n",
    "        \n",
    "        # Add a batch dimension, pass the image through the model to get the embeddings\n",
    "        img_enc = model(img.unsqueeze(0))\n",
    "        \n",
    "        # Detach the embeddings from the computation graph and move them back to the CPU\n",
    "        img_enc = img_enc.detach().cpu().numpy()\n",
    "        \n",
    "        # Convert the embeddings to a NumPy array\n",
    "        img_enc = np.array(img_enc)\n",
    "\n",
    "    return img_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchInDatabase(img_enc, database):\n",
    "    \"\"\"\n",
    "    Searches for the closest match to a given image embedding in a database of embeddings.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img_enc (numpy.ndarray): the embedding of the input image.\n",
    "    database (pandas.DataFrame): a DataFrame containing image embeddings with 'anchor' column for image names and subsequent columns for embedding values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (str): the name of the anchor image in the database that is closest to the input image embedding.\n",
    "    \"\"\"\n",
    "    # Extract the embeddings and image names from the database\n",
    "    anc_enc_arr = database.iloc[:, 1:].to_numpy()  # All columns except the first one (assuming embeddings start from the second column)\n",
    "\n",
    "    # Initialize an empty list to store distances\n",
    "    distance = []\n",
    "\n",
    "    # Iterate over each embedding in the database\n",
    "    for i in range(anc_enc_arr.shape[0]):\n",
    "        # Compute the Euclidean distance between the input embedding and the current database embedding\n",
    "        dist = euclidean_dist(img_enc, anc_enc_arr[i : i+1, :])\n",
    "        # Append the computed distance to the distance list\n",
    "        distance = np.append(distance, dist)\n",
    "\n",
    "    # Find the index of the closest embedding by sorting distances\n",
    "    closest_idx = np.argsort(distance)\n",
    "\n",
    "    # Return the name of the closest anchor image\n",
    "    return database['anchor'][closest_idx[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_fn(test_df, currentTest, y_pred, model, database):\n",
    "    \"\"\"\n",
    "    Tests model predictive capability.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        test_df (pandas.DataFrame): DataFrame containing the test images.\n",
    "        currentTest (str): desired images to test the model on.\n",
    "        y_pred (list): a list to store the predictions ('real' or 'fake') for each test image.\n",
    "        model (torch.nn.Module): the PyTorch model used to generate the image embeddings.\n",
    "        database (pandas.DataFrame): the DataFrame containing image embeddings with 'Anchor' column for image names and subsequent columns for embedding values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Iterate over each row in the test DataFrame with progress bar\n",
    "    for index, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "        # Construct the full image path\n",
    "        img_name = DATA_DIR + row[currentTest]\n",
    "        \n",
    "        # Read the image from the path\n",
    "        img = io.imread(img_name)\n",
    "\n",
    "        # Get the image embeddings using the model\n",
    "        img_enc = getImageEmbeddings(img, model)\n",
    "\n",
    "        # Search for the closest match in the database\n",
    "        closestLabel = searchInDatabase(img_enc, database)\n",
    "\n",
    "        # Determine if the closest match is 'real' or 'fake' and append the result to pred_vector\n",
    "        if \"real\" in closestLabel:\n",
    "            y_pred.append(\"real\")\n",
    "        else:\n",
    "            y_pred.append(\"fake\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(test_df,y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Calculate and display various classification metrics along with the confusion matrix.\n",
    "\n",
    "    This function takes prediction results and ground truth values, computes a confusion matrix,\n",
    "    classification report, and several key metrics including accuracy, precision, recall, and F1-score,\n",
    "    and then visualizes the confusion matrix using a heatmap.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    test_df : DataFrame\n",
    "        DataFrame containing the test data, used to determine the size of the ground truth array.\n",
    "    y_pred : array-like\n",
    "        Predicted labels for the test set.\n",
    "    y_true : array-like\n",
    "        True labels for the test set; overridden internally based on the `test_df`.\n",
    "    \"\"\"\n",
    "    #Ground truth vectors creation\n",
    "    y_true = np.array(['real'] * len(test_df))\n",
    "    temp = np.array(['fake'] * len(test_df))\n",
    "    y_true = np.concatenate([y_true, temp])\n",
    "\n",
    "    # Confusion Matrix calculation\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[\"real\",\"fake\"])\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', xticklabels=[\"Real\", \"Fake\"], yticklabels=[\"Real\", \"Fake\"])\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.ylabel('True labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    # Classification Report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "    # True Negative, False Positive, False Negative, True Positive extraction\n",
    "    TN, FP, FN, TP = confusion_matrix(y_true, y_pred, labels=[\"real\", \"fake\"]).ravel()\n",
    "    print(f\"Numero di veri negativi {TN}\")\n",
    "    print(f\"Numero di veri negativi {FP}\")\n",
    "    \n",
    "    # Metrics calculation\n",
    "    accuracy = round((TP + TN) /(TP + TN + FP + FN), 4) * 100\n",
    "    precision = round((TP) / (TP + FP), 4) * 100\n",
    "    sensitivy_recall = round((TP) / (TP + FN), 4) * 100\n",
    "    specificity = round((TN) / (TN + FP) * 100, 4)\n",
    "    F1_score = round((2* precision * sensitivy_recall) / (precision + sensitivy_recall), 2)\n",
    "\n",
    "    print({\"Accuracy\":accuracy,\"Precision\":precision,\"Sensitivity_recall\":sensitivy_recall, \"Specificity\": specificity, \"F1_score\":F1_score})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GENERAL PERFORMANCE TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing prep\n",
    "y_true = []\n",
    "y_pred = []\n",
    "tempDf = test_df\n",
    "database = df_enc\n",
    "\n",
    "\n",
    "# Testing on FAKE images\n",
    "prediction_fn(tempDf, 'real', y_pred, model, database)\n",
    "\n",
    "# Testing on REAL images\n",
    "prediction_fn(tempDf, 'fake', y_pred, model, database)\n",
    "\n",
    "# Metrics calculation\n",
    "calculate_metrics(tempDf, y_pred, y_true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INDIVIDUAL FAKE GENERATOR PERFORMANCE TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_testSet_dirs(test_df):\n",
    "    \"\"\"\n",
    "    Extracts and returns unique generator directories for real and fake images from a test DataFrame.\n",
    "\n",
    "    This function parses the 'real' and 'fake' columns of the provided DataFrame to extract the top-level\n",
    "    directories, which are assumed to be the generator names. These names are collected, made unique, and\n",
    "    returned as sorted lists.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    test_df : DataFrame\n",
    "        The DataFrame containing test data with columns 'real' and 'fake' that include file paths.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple of list\n",
    "        A tuple containing two lists: the first list contains unique directories of real images,\n",
    "        and the second list contains unique directories of fake images.\n",
    "    \"\"\"\n",
    "    real_dirs_list = []\n",
    "    fake_dirs_list = []\n",
    "\n",
    "    # Iterate through each row of the DataFrame to extract generator directories\n",
    "    for index, row in tqdm(test_df.iterrows(), total=test_df.shape[0], desc=\"Extracting generator directories\"):\n",
    "        # Extract the generator directory from the 'real' path\n",
    "        r_dir = row[\"real\"].split('fourier\\\\fourier\\\\')[1].split('+')[0]\n",
    "        # Extract the generator directory from the 'fake' path\n",
    "        f_dir = row[\"fake\"].split('fourier\\\\fourier\\\\')[1].split('+')[0]\n",
    "\n",
    "        # Append extracted directories to their respective lists\n",
    "        real_dirs_list.append(r_dir)\n",
    "        fake_dirs_list.append(f_dir)\n",
    "\n",
    "    # Remove duplicates and sort the directories\n",
    "    real_dirs = sorted(set(real_dirs_list))\n",
    "    fake_dirs = sorted(set(fake_dirs_list))\n",
    "\n",
    "    # Return the unique and sorted directories for real and fake images\n",
    "    return real_dirs, fake_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_testSet_subsets(test_df):\n",
    "    \"\"\"\n",
    "    Extracts subsets of real and fake images from the test DataFrame grouped by their generator directories.\n",
    "\n",
    "    This function first retrieves the list of real and fake generator directories from the test DataFrame. It then\n",
    "    filters and groups the images by these directories, creating subsets of image paths for each directory. Each subset\n",
    "    is converted into a DataFrame, which is then returned.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    test_df : DataFrame\n",
    "        The DataFrame containing test data with columns 'real' and 'fake' that include file paths.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple of dicts\n",
    "        A tuple containing two dictionaries: the first dictionary maps real image directories to DataFrames of their image paths,\n",
    "        and the second dictionary maps fake image directories to DataFrames of their image paths.\n",
    "    \"\"\"\n",
    "    # Retrieve directories for real and fake images\n",
    "    real_dirs, fake_dirs = get_testSet_dirs(test_df)\n",
    "    print(real_dirs)\n",
    "    print(fake_dirs)\n",
    "\n",
    "    # Create subsets for real images grouped by their generator directories\n",
    "    real_subsets = {\n",
    "        dir: test_df[test_df['real'].str.startswith(\"fourier\\\\fourier\\\\\" + dir + '+')]['real'].tolist()\n",
    "        for dir in real_dirs\n",
    "    }\n",
    "    \n",
    "    # Create subsets for fake images grouped by their generator directories\n",
    "    fake_subsets = {\n",
    "        dir: test_df[test_df['fake'].str.startswith(\"fourier\\\\fourier\\\\\" + dir + '+')]['fake'].tolist()\n",
    "        for dir in fake_dirs\n",
    "    }\n",
    "\n",
    "    # Convert lists of image paths into DataFrames for each generator directory\n",
    "    real_subsets_df = {\n",
    "        dir: pd.DataFrame(subset, columns=['real'])\n",
    "        for dir, subset in real_subsets.items()\n",
    "    }\n",
    "    \n",
    "    fake_subsets_df = {\n",
    "        dir: pd.DataFrame(subset, columns=['fake'])\n",
    "        for dir, subset in fake_subsets.items()\n",
    "    }\n",
    "\n",
    "    # Return dictionaries of DataFrames for real and fake images grouped by generator directories\n",
    "    return real_subsets_df, fake_subsets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HERE ARE CALCULATED INDIVIDUAL METRICS FOR EACH FAKE GENERATOR\n",
    "real_subsets, fake_subsets = get_testSet_subsets(test_df)\n",
    "\n",
    "# Lenght matching \n",
    "first_real_key = next(iter(real_subsets))\n",
    "first_fake_key = next(iter(fake_subsets))\n",
    "\n",
    "real_df = real_subsets[first_real_key]\n",
    "first_fake_df = fake_subsets[first_fake_key] \n",
    "\n",
    "real_df = real_df.head(len(first_fake_df))\n",
    "\n",
    "# Testing prep\n",
    "y_pred_real = [] #Avoid redoing predictions on the same real images subset\n",
    "y_pred = []\n",
    "database = df_enc\n",
    "\n",
    "# Testing on REAL images\n",
    "prediction_fn(real_df, 'real', y_pred_real, model, database)\n",
    "\n",
    "for dir, df in fake_subsets.items():\n",
    "    print(f\"\\n\\n\\nMetrics summary for {dir}:\")\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = y_pred_real.copy()\n",
    "\n",
    "    # Testing on FAKE images\n",
    "    prediction_fn(df, 'fake', y_pred, model, database)\n",
    "    # Metrics calculation\n",
    "    # (the df in input makes no differences)\n",
    "    calculate_metrics(df, y_pred, y_true)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepfake",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
