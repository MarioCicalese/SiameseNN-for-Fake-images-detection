{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from constants import DatasetPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utility constants\n",
    "DATASET_PATH = DatasetPath.effectivePath\n",
    "\n",
    "def DATASET_DIRS():\n",
    "\treturn os.listdir(DATASET_PATH)\n",
    "\n",
    "REAL_DIRS = [\"afhq\", \"celebahq\", \"coco\", \"ffhq\", \"imagenet\", \"landscape\", \"lsun\", \"metfaces\", \"cycle_gan\"]\n",
    "\n",
    "FAKE_DIRS = [\"big_gan\", \"cips\", \"cycle_gan\", \"ddpm\", \"denoising_diffusion_gan\", \"diffusion_gan\", \"face_synthetics\", \n",
    "\t\t\t\t \"gansformer\", \"gau_gan\", \"generative_inpainting\", \"glide\", \"lama\", \"latent_diffusion\", \"mat\", \"palette\", \n",
    "\t\t\t\t \"pro_gan\", \"projected_gan\", \"sfhq\", \"stable_diffusion\", \"star_gan\", \"stylegan1\", \"stylegan2\", \"stylegan3\",\n",
    "\t\t\t\t \"taming_transformer\", \"vq_diffusion\"]\n",
    "\n",
    "csv_columns_name = ['filename', 'image_path', 'target', 'category']\n",
    "\n",
    "\n",
    "DIR_FOURIER_PATH = DATASET_PATH + \"fourier\\\\\"\n",
    "FFTS_PATH = DIR_FOURIER_PATH + \"fourier\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_csv(df, filename, path):\n",
    "\t\"\"\"\n",
    "\tSplits the DataFrame in chunks to enable tqdm progress visualization while converting the DataFrame into a '.csv' file.\n",
    "\n",
    "\tParametres\n",
    "\t----------\n",
    "\t\tdf (pd.DataFrame): the DataFrame to convert.\n",
    "\t\tfilename (str): the desired file name (comprehensive of '.csv' extension).\n",
    "\t\tpath (str): the path where the '.csv' will be stored.\n",
    "\t\"\"\"\n",
    "\tchunks = np.array_split(df.index, 100)\n",
    "\tfor chunck, subset in enumerate(tqdm(chunks, desc=\"Creating \\'\" + filename + \"\\' file\")):\n",
    "\t\tif chunck == 0: # first row\n",
    "\t\t\tdf.loc[subset].to_csv(path, mode='w', index=False)\n",
    "\t\telse:\n",
    "\t\t\tdf.loc[subset].to_csv(path, header=None, mode='a', index=False)\n",
    "\n",
    "\tprint(\"\\'\" + filename + \"\\' has been successfully created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(\"fourier\" in DATASET_DIRS()):\n",
    "\tprint(\"Fourier folder already exist.\")\n",
    "else:\n",
    "\tmode = 0o666\n",
    "\tpath = os.path.join(DATASET_PATH,\"fourier\")\n",
    "\t#creates the fourier folder in the main Dataset folder\n",
    "\tos.mkdir(path,mode)\n",
    "\t\n",
    "\t#creates a fourier folder in the fourier folder\n",
    "\tos.mkdir(path + \"\\\\fourier\", mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greyscale_FFT(img_path):\n",
    "\t\"\"\"\n",
    "\tApplies Fast Fourier Transform (FFT) to a greyscale image and returns its magnitude spectrum.\n",
    "\n",
    "\tParameters\n",
    "\t----------\n",
    "\t\timg_path (str): input image path.\n",
    "\n",
    "\tReturns\n",
    "\t-------\n",
    "\t\tfft_img (np.ndarray): a 2D array representing the magnitude spectrum of the FFT of the input image, normalized to the range [0, 255].\n",
    "\t\"\"\"\n",
    "\t\n",
    "\t# Read the image from the specified path in BGR color format\n",
    "\tRGBimg = cv2.imread(img_path)\n",
    "\t\n",
    "\t# Convert the image from BGR to grayscale\n",
    "\tgrayImg = cv2.cvtColor(RGBimg, cv2.COLOR_BGR2GRAY)\n",
    "\t\n",
    "\t# Apply the 2D FFT to the grayscale image\n",
    "\tfft_img = np.fft.fft2(grayImg)\n",
    "\t\n",
    "\t# Compute the logarithm of the absolute value of the FFT to get the magnitude\n",
    "\tfft_img = np.log(np.abs(fft_img))\n",
    "\n",
    "\t# Find the minimum and maximum values of the magnitude for normalization\n",
    "\tmin_val = np.min(fft_img)\n",
    "\tmax_val = np.max(fft_img)\n",
    "\t\n",
    "\t# Normalize the magnitude image to the range [0, 255]\n",
    "\tfft_img = (fft_img - min_val) * (255.0 / (max_val - min_val))\n",
    "\t\n",
    "\t# Convert the normalized image to uint8 (integer values from 0 to 255)\n",
    "\tfft_img = np.uint8(fft_img)\n",
    "\n",
    "\t# Return the normalized magnitude image\n",
    "\treturn fft_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FFT_application(ds_partition_df):\n",
    "\t\"\"\"\n",
    "\tApplies Fast Fourier Transform (FFT) to the images in the given dataset.\n",
    "\tThe function also saves resulting images to the proper directories and generates a 'metadata.csv' for utility.\n",
    "\n",
    "\tParameters\n",
    "\t----------\n",
    "\t\tds_partition_df (pd.DataFrame): the DataFrame containing dataset partition information with columns 'image_path', 'filename', 'target', and 'category'.\n",
    "\n",
    "\tReturns\n",
    "\t-------\n",
    "\t\tfilename_matching_dict (dict): Dictionary mapping original filenames to their adjusted filenames based on the target value.\n",
    "\t\tpath_matching_dict (dict): Dictionary mapping original image paths to their Fourier transformed paths.\n",
    "\t\"\"\"\n",
    "\t# DataFrame to store Fourier metadata\n",
    "\tfourier_metadata_df = pd.DataFrame(columns=csv_columns_name)\n",
    "\t\n",
    "\t# Dictionaries to store mappings\n",
    "\tpath_matching_dict = {}\n",
    "\tfilename_matching_dict = {}\n",
    "\n",
    "\t# Iterate through the dataset partition DataFrame\n",
    "\tfor index, row in tqdm(ds_partition_df.iterrows(), total=ds_partition_df.shape[0], desc=\"FFT application\"):\n",
    "\t\t# Construct the full path to the image\n",
    "\t\tpath = DATASET_PATH + row[\"image_path\"]\n",
    "\t\t\n",
    "\t\t# Generate the Fourier transformed image\n",
    "\t\tfft_img = greyscale_FFT(path)\n",
    "\t\t\n",
    "\t\t# Adjust the filename based on the target value\n",
    "\t\tfilename = row[\"filename\"]\n",
    "\t\tif row[\"target\"] == 0:\n",
    "\t\t\tadjusted_filename = filename.replace(\"img\", \"real\")\n",
    "\t\telse:\n",
    "\t\t\tadjusted_filename = filename.replace(\"img\", \"fake\")\n",
    "\n",
    "\t\t# Construct the path to save the Fourier transformed image\n",
    "\t\tfft_path = FFTS_PATH + adjusted_filename\n",
    "\t\tcv2.imwrite(fft_path, fft_img)\n",
    "\t\t\n",
    "\t\t# Get the relative path of the Fourier transformed image\n",
    "\t\tsplit_fft_path = fft_path.split(DATASET_PATH)[-1]\n",
    "\n",
    "\t\t# Add metadata to the DataFrame\n",
    "\t\tfourier_metadata_df.loc[len(fourier_metadata_df)] = [adjusted_filename, split_fft_path, row[\"target\"], row[\"category\"]]\n",
    "\t\t\n",
    "\t\t# Update the dictionaries\n",
    "\t\tfilename_matching_dict[filename] = adjusted_filename\n",
    "\t\tpath_matching_dict[row[\"image_path\"]] = split_fft_path\n",
    "\t\n",
    "\t# Save 'metadata.csv'\n",
    "\tdf_to_csv(fourier_metadata_df, \"metadata.csv\", DIR_FOURIER_PATH + \"metadata.csv\")\n",
    "\n",
    "\treturn filename_matching_dict, path_matching_dict\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dirs(ds_partition_df):\n",
    "\t\"\"\"\n",
    "\tRetrieves image source directories from the dataset.\n",
    "\n",
    "\tParameters\n",
    "\t----------\n",
    "\t\tds_partition_df (pandas.DataFrame): the dataframe containing the dataset partition.\n",
    "\tReturns\n",
    "\t-------\n",
    "\t\treal_dirs (set of str): a set of unique directories that contain real images.\n",
    "\t\tfake_dirs (set of str): a set of unique directories that contain fake images.\n",
    "\t\"\"\"\n",
    "\t# Initialize lists to store directories for real and fake images\n",
    "\treal_dirs_list = []\n",
    "\tfake_dirs_list = []\n",
    "\n",
    "\t# Iterate through each row in the dataframe with a progress bar\n",
    "\tfor index, row in tqdm(ds_partition_df.iterrows(), total=ds_partition_df.shape[0]):\n",
    "\t\t# Extract the top-level directory from the image path\n",
    "\t\tdir = row[\"image_path\"].split('/')[0]\n",
    "\n",
    "\t\t# Categorize the directory as 'real' or 'fake' based on its presence in REAL_DIRS\n",
    "\t\tif dir in REAL_DIRS:\n",
    "\t\t\treal_dirs_list.append(dir)\n",
    "\t\telse:\n",
    "\t\t\tfake_dirs_list.append(dir)\n",
    "\n",
    "\treal_dirs = sorted(set(real_dirs_list))\n",
    "\tfake_dirs = sorted(set(fake_dirs_list))\n",
    "\n",
    "\t# Return the lists of real and fake directories\n",
    "\treturn real_dirs, fake_dirs\n",
    "\n",
    "\n",
    "\n",
    "def custom_train_test_split(ds_partition_df, filename_matching_dict, path_matching_dict):\n",
    "\t\"\"\"\n",
    "\tSplits a dataset partition in training set and test set while keeping a 1:1 ratio between Real and Fake images.\n",
    "\tIt also ensure equal proportion for images from different directories. \n",
    "\n",
    "\tParametres\n",
    "\t----------\n",
    "\t\tds_partition_df (pandas.DataFrame): DataFrame containing the dataset partition\n",
    "\t\tfilename_matching_dict (dict): Dictionary mapping original filenames to their adjusted filenames based on the target value.\n",
    "\t\tpath_matching_dict (dict): Dictionary mapping original image paths to their Fourier transformed paths.\n",
    "\t\n",
    "\tReturns\n",
    "\t-------\n",
    "\t\ttrain_df, test_df (pandas.DataFrame): Training and Test Sets DataFrames\n",
    "\t\"\"\"\n",
    "\t\n",
    "\t# Initialization\n",
    "\tsize = len(ds_partition_df)\n",
    "\treal_dirs, fake_dirs = get_dirs(ds_partition_df)\n",
    "\tprint(real_dirs)\n",
    "\tprint(fake_dirs)\n",
    "\ttrain_df = ds_partition_df\n",
    "\n",
    "\ttest_df = pd.DataFrame(columns=['real', 'fake'])\n",
    "\n",
    "\t# Number of tuples to sample from each real images directory\n",
    "\treal_sample_size = round(size / 2 * 0.2 / len(real_dirs))\n",
    "\ti = 1\n",
    "\t#Iterate through the dataset partition and sample the same number of tuples for each fake images directory\n",
    "\tfor dir in tqdm(real_dirs, desc=\"Test set Real images sampling\"):\n",
    "\t\tsampled_size = 0\n",
    "\t\tfor index, row in train_df.iterrows():\n",
    "\t\t\tif sampled_size < real_sample_size:\n",
    "\t\t\t\timage_path = row[\"image_path\"]\n",
    "\t\t\t\tsplitted_path = image_path.split('/')[0]\n",
    "\t\t\t\tif dir==splitted_path:\n",
    "\t\t\t\t\ttest_df.loc[i] = [path_matching_dict[image_path], None]\n",
    "\t\t\t\t\ttrain_df = train_df.drop(index=index)\n",
    "\t\t\t\t\tsampled_size+=1\n",
    "\t\t\t\t\ti+=1\n",
    "\t\t\telse: break\n",
    "\t\n",
    "\t# Number of tuples to sample from each fake images directory\n",
    "\tfake_sample_size = round(size / 2 * 0.2 / len(fake_dirs))\n",
    "\ti = 1\n",
    "\t#Iterate through the dataset partition and sample the same number of tuples for each fake images directory\n",
    "\tfor dir in tqdm(fake_dirs, desc=\"Test set Fake images sampling\"):\n",
    "\t\t\tsampled_size = 0\n",
    "\t\t\tfor index, row in train_df.iterrows():\n",
    "\t\t\t\t\tif sampled_size < fake_sample_size:\n",
    "\t\t\t\t\t\t\timage_path = row[\"image_path\"]\n",
    "\t\t\t\t\t\t\tsplitted_path = image_path.split('/')[0]\n",
    "\t\t\t\t\t\t\tif dir == splitted_path:\n",
    "\t\t\t\t\t\t\t\t\ttest_df.loc[i] = [test_df.loc[i][\"real\"], path_matching_dict[image_path]]\n",
    "\t\t\t\t\t\t\t\t\ttrain_df = train_df.drop(index=index)\n",
    "\t\t\t\t\t\t\t\t\tsampled_size+=1\n",
    "\t\t\t\t\t\t\t\t\ti+=1\n",
    "\n",
    "\t\t\t\t\telse: break\n",
    "\t\n",
    "\t# Update training set \n",
    "\tfor index, row in train_df.iterrows():\n",
    "\t\t\n",
    "\t\ttrain_df.at[index, \"filename\"] = filename_matching_dict[row[\"filename\"]]\n",
    "\t\ttrain_df.at[index, \"image_path\"] = path_matching_dict[row[\"image_path\"]]\n",
    "\n",
    "\n",
    "\treturn train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_test_sets(ds_partition_path):\n",
    "\t\n",
    "\tds_partition_df = pd.read_csv(ds_partition_path)\n",
    "\n",
    "\tfilename_matching_dict, path_matching_dict = FFT_application(ds_partition_df)\n",
    "\t\n",
    "\ttrain_df, test_df = custom_train_test_split(ds_partition_df, filename_matching_dict, path_matching_dict)\n",
    "\n",
    "\tdf_to_csv(train_df, \"trainingSet.csv\", DATASET_PATH + \"trainingSet.csv\")\n",
    "\tdf_to_csv(test_df, \"testSet.csv\", DATASET_PATH + \"testSet.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_training_test_sets(DATASET_PATH + \"dataset_partition.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate csv file that will be used to train the model\n",
    "\n",
    "input_df = pd.DataFrame(columns= [\"anchor\",\"positive\",\"negative\"])\n",
    "\n",
    "fourier_df = pd.read_csv(DATASET_PATH + \"trainingSet.csv\")\n",
    "input_df[\"anchor\"] = fourier_df[\"image_path\"]\n",
    "\n",
    "real_fourier_df = fourier_df[ fourier_df[\"target\"] == 0 ]\n",
    "fake_fourier_df = fourier_df[ fourier_df[\"target\"] != 0 ]\n",
    "\n",
    "\n",
    "for index, row in input_df.iterrows():\n",
    "\timage_path = row[\"anchor\"]\n",
    "\t\n",
    "\tif \"real\" in image_path:\n",
    "\t\tpositive_img = real_fourier_df.sample(frac=1).head(1)\n",
    "\t\twhile positive_img[\"image_path\"].iloc[0] == image_path:\n",
    "\t\t\tpositive_img = real_fourier_df.sample(frac=1).head(1)\n",
    "\t\tinput_df.at[index, \"positive\"] = positive_img[\"image_path\"].iloc[0]\n",
    "\t\tnegative_img = fake_fourier_df.sample(frac=1).head(1)\n",
    "\t\tinput_df.at[index, \"negative\"] = negative_img[\"image_path\"].iloc[0]\n",
    "\telif \"fake\" in image_path:\n",
    "\t\tpositive_img = real_fourier_df.sample(frac=1).head(1)\n",
    "\t\tinput_df.at[index, \"negative\"] = positive_img[\"image_path\"].iloc[0]\n",
    "\t\tnegative_img = fake_fourier_df.sample(frac=1).head(1)\n",
    "\t\twhile negative_img[\"image_path\"].iloc[0] == image_path:\n",
    "\t\t\tnegative_img = fake_fourier_df.sample(frac=1).head(1)\n",
    "\t\tinput_df.at[index, \"positive\"] = negative_img[\"image_path\"].iloc[0]\n",
    "\t\t\t\n",
    "\n",
    "input_df.to_csv(DATASET_PATH + \"input.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DETECTIVE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
